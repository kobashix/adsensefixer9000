from __future__ import annotations

import json
import math
import re
from typing import Dict, List

from bs4 import BeautifulSoup

from gpvb.detect.text import cluster_simhash, word_count
from gpvb.models import Finding, FindingCategory, PageResult, Severity


AUTO_CONTENT_POLICY_LINKS = [
    "https://support.google.com/adsense/answer/1348737",
    "https://support.google.com/webmasters/answer/2721306",
]


def _word_entropy(text: str) -> float:
    words = re.findall(r"\w+", text.lower())
    if not words:
        return 0.0
    total = len(words)
    counts: Dict[str, int] = {}
    for word in words:
        counts[word] = counts.get(word, 0) + 1
    entropy = 0.0
    for count in counts.values():
        p = count / total
        entropy -= p * math.log2(p)
    return entropy


def _unique_sentence_ratio(text: str) -> float:
    sentences = [s.strip() for s in re.split(r"[.!?]+", text) if s.strip()]
    if not sentences:
        return 1.0
    return len(set(sentences)) / len(sentences)


def _has_author_or_date(html: str) -> bool:
    soup = BeautifulSoup(html, "lxml")
    if soup.find("meta", attrs={"name": re.compile("author", re.I)}):
        return True
    if soup.find("time"):
        return True
    for script in soup.find_all("script", attrs={"type": "application/ld+json"}):
        try:
            data = json.loads(script.get_text(strip=True))
        except json.JSONDecodeError:
            continue
        serialized = json.dumps(data).lower()
        if "author" in serialized or "datepublished" in serialized:
            return True
    return False


def detect_autogenerated_findings(page: PageResult) -> List[Finding]:
    findings: List[Finding] = []
    text = page.text or ""
    entropy = _word_entropy(text)
    sentence_ratio = _unique_sentence_ratio(text)
    boilerplate_ratio = len(text) / max(len(page.html), 1)
    signals = {
        "entropy": round(entropy, 3),
        "unique_sentence_ratio": round(sentence_ratio, 3),
        "boilerplate_ratio": round(boilerplate_ratio, 3),
        "word_count": word_count(text),
    }

    if entropy < 3.0 or sentence_ratio < 0.5 or boilerplate_ratio < 0.05:
        findings.append(
            Finding(
                detector="autogenerated_single_page_signals",
                severity=Severity.medium,
                category=FindingCategory.program_policy,
                confidence=0.58,
                message="Text signals suggest auto-generated or low-quality content.",
                remediation=[
                    "Add original, human-written content with unique structure and detail.",
                    "Reduce template boilerplate and include author/date metadata.",
                ],
                policy_links=AUTO_CONTENT_POLICY_LINKS,
                evidence=signals,
            )
        )
    return findings


def apply_autogenerated_findings(pages: List[PageResult], threshold: float = 0.9) -> None:
    urls = [page.url for page in pages]
    texts = [page.text for page in pages]
    clusters = cluster_simhash(urls, texts, threshold)

    url_to_page = {page.url: page for page in pages}
    for cluster_urls, similarity in clusters:
        for url in cluster_urls:
            page = url_to_page.get(url)
            if not page:
                continue
            if word_count(page.text) < 150 and not _has_author_or_date(page.html):
                page.findings.append(
                    Finding(
                        detector="autogenerated_cluster_content",
                        severity=Severity.high,
                        category=FindingCategory.program_policy,
                        confidence=0.74,
                        message="Clustered pages resemble template or scraped content.",
                        remediation=[
                            "Replace templated pages with unique, authored content.",
                            "Include author and date metadata for original work.",
                        ],
                        policy_links=AUTO_CONTENT_POLICY_LINKS,
                        evidence={
                            "cluster_urls": cluster_urls,
                            "similarity": round(similarity, 3),
                        },
                    )
                )

    similarity_clusters = cluster_simhash(urls, texts, 0.7)
    for cluster_urls, similarity in similarity_clusters:
        if len(cluster_urls) < 2:
            continue
        for url in cluster_urls:
            page = url_to_page.get(url)
            if not page:
                continue
            if word_count(page.text) < 150 and not _has_author_or_date(page.html):
                page.findings.append(
                    Finding(
                        detector="autogenerated_similarity_pattern",
                        severity=Severity.medium,
                        category=FindingCategory.program_policy,
                        confidence=0.55,
                        message="Page content is short and highly similar to other pages.",
                        remediation=[
                            "Increase unique content and avoid duplicating templates.",
                            "Add author/date metadata to reinforce content ownership.",
                        ],
                        policy_links=AUTO_CONTENT_POLICY_LINKS,
                        evidence={
                            "similarity": round(similarity, 3),
                            "cluster_urls": cluster_urls,
                        },
                    )
                )
